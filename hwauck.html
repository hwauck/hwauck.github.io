<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Helen Wauck</title>
    <link rel="stylesheet" href="./hwauck.css">
    <link rel="icon" href="./favicon.ico" type="image/x-icon">
  </head>
  <body>
    <main>
        <h1>Helen Wauck</h1>  
        <h2>UX Researcher</h2> 
        <div class="grid-container">
          <div class="me">PROFILE PIC</div>
          <div class="grid-item">UX Researcher with 3 years of experience in industry and 6 in academia getting my PhD in Human-Computer Interaction. As the lead UX researcher in every industry and academic project I've worked on, I develop and execute UX research plans across the entire user-centered, iterative design cycle for a variety of products with very different target users. Some of these include cancer doctors and patients, students studying STEM in grade school through college, AI plan recognition engineering scientists, and automated drone operators, among others.   

            

<!-- 
             remote and in-person usability testing, in-depth interviews, surveys, quantitative metrics development, statistical analysis, qualitative thematic analyses, and the sort of user empathy required to advocate for diversity, equity, and inclusion. -->
             </div>
 
        </div> 
        <div>Selected Projects</div>
        <div>Activity Recognition Software for Remote Patient Care</div>
        <div>Project Summary: A web application for cancer doctors, nurse practitioners, and physical therapists to assess their patients' health remotely by analyzing the way patients performed assigned physical therapy exercises at home. This required patients to record video of themselves in their home performing the exercises. While this sounds invasive to patients' privacy, the core concept of the product was to use AI to recognize and analyze the exercise being performed from the video and then send only the analysis - not the video - to the physician.</div>
        <div>My Role: Lead the UX research and UI design process from initial target user interviews to functional prototype. </div>
        <div>Initial interviews: Before beginning any UI design work, I interviewed four physicians involved in cancer care and physical therapy to understand their current practices when assigning physical therapy exercises to their patients and monitoring their progress. The main takeaways from these interviews were that 1) assessing patient exercise performance is very tailored to the individual and 2) Getting patients to do something rather than nothing is the priority rather than following a strict exercise regimen.</div>
          <div>Usability tests and design iteration: one of the overarching challenges with this project was getting a sufficient sample size for each of the usability tests. Physicians have huge demands on their time and their priorities are to their patients, not external researchers who they don't know. To address these difficulties, the project team took advantage of our subject matter expert's connections to other physicians in their network; he was able to evangelize the study to his colleagues to encourage their participation and avoid pestering them. While this was a snowball sample and therefore less representative, it maximized our chances of getting even just a few clinicians to participate. <div>
            <div>The interface design I used for the first usability test was a low fidelity "paper" prototype, but adapted to the constraints of remote testing via Zoom, our only option during the pandemic. The adapted prototype consisted of a PowerPoint deck with slide transitions on click and hover, allowing for limited interactivity. The core feature of the prototype was a set of calendars showing the system's day to day activity analysis for each exercise a patient was assigned. Each calendar used various categories for each day's activity analysis, conveyed via a combination of color, symbology, and tooltip text: doing the exercise correctly, doing the exercise incorrectly, and not doing the exercise at all. The interface also displayed the system's confidence level in its analysis for each day </div>
            <div>As recruiting clinicians was difficult, just 2 participated in the first usability test. I gave them 8 tasks to complete in 30 minutes and used a standard think-aloud protocol while observing them perform the tasks. Since the participants could not interact with the prototype directly, I asked them to tell me what actions they would perform on the interface and executed them myself. They had some difficulties figuring out where in the interface they needed to navigate to complete a task, and misinterpreted the output of the system's exercise analysis (which was populated with data from fake patients), thinking that it represented patient self-reports. 
              
              
              In hindsight, these problems probably could have been detected by running participants through a wireframe design usability test before the paper prototype test. Lesson learned for me on why wireframes are important.</div>
            <div>The high fidelity prototype consisted of a React app with a database of fake patient data on the backend. I ran the next usability test with this prototype and gave participants the same 8 tasks. As before, we only managed to recruit 2 participants, and one of them was someone who had already participated in the previous usability test. This was unfortunate given that prior participation would make me and the interface familiar to the participant and therefore introducing bias that would likely make them less likely to give honest constructive feedback. The main insight I got from this usability test was that the system's exercise analysis output was still confusing in a different way: the combination of colors and symbology prevented participants from interpreting the output correctly. </div>
            <div>Our modifications to the interface after this test consisted of revising the color and symbology used based on participant feedback about their priorities (e.g. it's better to do a smaller amount of the exercise correctly than do more but incorrectly), and adding indicators that the user could hover over a calendar day to see its tooltip.</div>
            <div>The last usability test revealed that participants were now able to correctly interpret the system's activity analysis for each day in the calendars, but one of them, a doctor, commented that the interface layout does not align with their typical workflow. They also mentioned that this level of detail would be more appropriate for PTs, who could then provide a summary to the doctor. These two pieces of feedback were the most fundamental I'd gotten thus far despite 2 previous usability tests. The reasons I didn't get this kind of feedback in earlier usability tests were probably that 1) there weren't enough participants in each usability test, 2) I reused some participants across different tests, and 3) the fact that I didn't run my first test using a wireframe. In hindsight, given that we only had enough time to run 3 usability tests, I should have run the first with a wireframe, the second with the paper prototype, and only the final one with the high fidelity prototype. The lack of participants is a more difficult problem to address given our limited connections in the medical field, but perhaps we will have more opportunities in the future to establish these connections with the diversity of projects my company pursues.</div>
          
    </main>
	<script src="index.js"></script>
  </body>
</html>